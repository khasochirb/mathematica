<!DOCTYPE HTML>
<html>
<head>
    <title>Random Variables &amp; Distributions - International Math Hub</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="../../assets/css/first.css" />
    <link rel="stylesheet" href="../../assets/css/improvements.css" />
    <link rel="stylesheet" href="../../assets/css/topic-pages.css" />
    <style>
        /* Topic-specific overrides */
        body {
            background: var(--color-background-light) !important;
        }
        
        /* Ensure proper spacing */
        .topic-container {
            margin-top: var(--space-4);
        }
    </style>
</head>
<body class="is-preload">
    <div id="page-wrapper">
        <!-- Header -->
        <section id="header">
            <div class="header-container" style="display: flex; align-items: center; justify-content: space-between; padding: 0 5%;">
                <div class="logo">
                    <a href="../../index.html">
                        <img src="../../images/logo.png" alt="International Math Hub" style="max-height: 120px;">
                    </a>
                </div>
                                <!-- Nav -->
                <nav id="nav">
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="../../features.html">Mathematics Roadmap</a></li>
                        <li><a href="../../resources.html">Resources</a></li>
                        <li><a href="../../blog.html">Blog</a></li>
                        <li><a href="../../contact.html">Contact</a></li>
                    </ul>
                </nav>
            </div>
        </section>

        <!-- Main -->
        <section id="main" class="edu-section">
            <div class="edu-container">
                <header class="text-center mb-6">
                <h2>Random Variables &amp; Distributions</h2>
                <p class="text-secondary">Mathematical foundations for modeling and analyzing random phenomena</p>
            </header>

            <div class="edu-card">
                <nav class="breadcrumb">
                    <a href="../../index.html">Home</a> &raquo;
                    <a href="../../resources.html">Resources</a> &raquo;
                    <a href="../../features.html">Mathematics Roadmap</a> &raquo;
                    <a href="../../features.html#probability">Probability</a> &raquo;
                    <span>Random Variables &amp; Distributions</span>
                </nav>
            </div>

                            </div>
                <div class="topic-container">
                <div class="topic-section edu-card">
                    <h3 class="topic-header">Introduction to Random Variables</h3>
                    
                    <p>A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. Random variables provide a way to translate complex random events into numbers that can be analyzed mathematically.</p>
                    
                    <h4>Definition and Types</h4>
                    <p>Formally, a random variable X is a function that assigns a real number to each outcome in the sample space of a random experiment. Random variables come in two main types:</p>
                    
                    <ul>
                        <li><strong>Discrete Random Variables:</strong> Take on a countable number of distinct values. Examples include the number of heads in coin tosses, the number of customers arriving at a store, or the number of defective items in a batch.</li>
                        <li><strong>Continuous Random Variables:</strong> Can take on any value in an interval or collection of intervals. Examples include height, weight, time, and temperature.</li>
                    </ul>
                    
                    <div class="example">
                        <div class="example-title">Example: Discrete Random Variable</div>
                        <p>In a traditional Mongolian shagai game, let X represent the number of sheep obtained when throwing four ankle bones. X is a discrete random variable that can take values from 0 to 4.</p>
                        <p>The probability distribution of X is:</p>
                        <p>P(X = 0) = (3/4)^4 = 81/256 (no sheep in any throw)</p>
                        <p>P(X = 1) = C(4,1) × (1/4) × (3/4)^3 = 108/256 (one sheep in any of the four throws)</p>
                        <p>P(X = 2) = C(4,2) × (1/4)^2 × (3/4)^2 = 54/256 (two sheep in any two throws)</p>
                        <p>P(X = 3) = C(4,3) × (1/4)^3 × (3/4) = 12/256 (three sheep in any three throws)</p>
                        <p>P(X = 4) = (1/4)^4 = 1/256 (sheep in all four throws)</p>
                    </div>
                    
                    <div class="example">
                        <div class="example-title">Example: Continuous Random Variable</div>
                        <p>Let Y represent the waiting time (in minutes) for a traditional Mongolian horse race to begin. Y is a continuous random variable that can take any non-negative real value.</p>
                        <p>For a continuous random variable, we typically work with probability density functions rather than individual probability values. If races begin according to a Poisson process with an average of 2 races per hour, the waiting time follows an exponential distribution with probability density function:</p>
                        <p>f(y) = (1/30)e^(-y/30) for y ≥ 0</p>
                        <p>The probability of waiting between 15 and 30 minutes would be the integral of this function from 15 to 30.</p>
                    </div>
                </div>
                
                <div class="topic-section edu-card">
                    <h3 class="topic-header">Probability Distributions</h3>
                    
                    <p>A probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes of a random variable.</p>
                    
                    <h4>Discrete Probability Distributions</h4>
                    <p>For a discrete random variable X, the probability distribution (also called probability mass function, PMF) gives the probability that X takes on a specific value x:</p>
                    <div class="math-formula">
                        P(X = x) = p(x)
                    </div>
                    <p>Properties of a valid PMF:</p>
                    <ul>
                        <li>p(x) ≥ 0 for all x (non-negativity)</li>
                        <li>Σ p(x) = 1 (sums to 1 over all possible values)</li>
                    </ul>
                    
                    <p>The cumulative distribution function (CDF) of a discrete random variable X gives the probability that X takes on a value less than or equal to x:</p>
                    <div class="math-formula">
                        F(x) = P(X ≤ x) = Σ p(t) for all t ≤ x
                    </div>
                    
                    <h4>Continuous Probability Distributions</h4>
                    <p>For a continuous random variable X, the probability distribution is characterized by a probability density function (PDF), denoted f(x). Unlike a PMF, a PDF does not directly give probabilities; instead, the probability that X takes on a value in a given interval [a, b] is:</p>
                    <div class="math-formula">
                        P(a ≤ X ≤ b) = ∫[a,b] f(x) dx
                    </div>
                    <p>Properties of a valid PDF:</p>
                    <ul>
                        <li>f(x) ≥ 0 for all x (non-negativity)</li>
                        <li>∫[-∞,∞] f(x) dx = 1 (integrates to 1 over the entire range)</li>
                    </ul>
                    
                    <p>The cumulative distribution function (CDF) of a continuous random variable X is:</p>
                    <div class="math-formula">
                        F(x) = P(X ≤ x) = ∫[-∞,x] f(t) dt
                    </div>
                    <p>And conversely, if F(x) is differentiable, then f(x) = dF(x)/dx.</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Uniform Distribution</div>
                        <p>A traditional Mongolian archer's arrow can land anywhere within a circular target of radius 1 meter. The distance from the center, X, follows a uniform distribution on [0, 1].</p>
                        <p>The probability density function is f(x) = 1 for 0 ≤ x ≤ 1, and f(x) = 0 otherwise.</p>
                        <p>The probability that an arrow lands between 0.3 and 0.7 meters from the center is:</p>
                        <p>P(0.3 ≤ X ≤ 0.7) = ∫[0.3,0.7] 1 dx = 0.7 - 0.3 = 0.4</p>
                    </div>
                </div>
                
                <div class="topic-section edu-card">
                    <h3 class="topic-header">Common Discrete Distributions</h3>
                    
                    <h4>Bernoulli Distribution</h4>
                    <p>The Bernoulli distribution models a single trial that can result in success (1) with probability p or failure (0) with probability 1-p.</p>
                    <div class="math-formula">
                        P(X = x) = p^x × (1-p)^(1-x) for x ∈ {0, 1}
                    </div>
                    <p>Mean: μ = p</p>
                    <p>Variance: σ² = p(1-p)</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Bernoulli Distribution</div>
                        <p>A fair coin is tossed. Let X = 1 if it lands heads, and X = 0 if it lands tails. Then X follows a Bernoulli distribution with p = 0.5.</p>
                        <p>P(X = 1) = 0.5, P(X = 0) = 0.5</p>
                        <p>Mean: μ = 0.5</p>
                        <p>Variance: σ² = 0.5 × (1 - 0.5) = 0.25</p>
                    </div>
                    
                    <h4>Binomial Distribution</h4>
                    <p>The binomial distribution models the number of successes in n independent Bernoulli trials, each with probability of success p.</p>
                    <div class="math-formula">
                        P(X = k) = C(n,k) × p^k × (1-p)^(n-k) for k = 0, 1, 2, ..., n
                    </div>
                    <p>where C(n,k) = n! / (k! × (n-k)!) is the binomial coefficient.</p>
                    <p>Mean: μ = np</p>
                    <p>Variance: σ² = np(1-p)</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Binomial Distribution</div>
                        <p>A fair coin is tossed 5 times. Let X represent the number of heads obtained. X follows a binomial distribution with n = 5 and p = 0.5.</p>
                        <p>The probability of getting exactly 3 heads is:</p>
                        <p>P(X = 3) = C(5,3) × 0.5^3 × 0.5^2 = 10 × 0.125 × 0.25 = 0.3125</p>
                        <p>Mean: μ = 5 × 0.5 = 2.5</p>
                        <p>Variance: σ² = 5 × 0.5 × 0.5 = 1.25</p>
                    </div>
                    
                    <h4>Poisson Distribution</h4>
                    <p>The Poisson distribution models the number of events occurring in a fixed interval of time or space, assuming these events occur with a known constant mean rate and independently of each other.</p>
                    <div class="math-formula">
                        P(X = k) = (λ^k × e^(-λ)) / k! for k = 0, 1, 2, ...
                    </div>
                    <p>where λ is the average number of events in the interval.</p>
                    <p>Mean: μ = λ</p>
                    <p>Variance: σ² = λ</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Poisson Distribution</div>
                        <p>A call center receives an average of 10 calls per hour. Assuming the calls follow a Poisson distribution, the probability of receiving exactly 8 calls in the next hour is:</p>
                        <p>P(X = 8) = (10^8 × e^(-10)) / 8! ≈ 0.1126</p>
                        <p>The probability of receiving more than 15 calls would need to be calculated as:</p>
                        <p>P(X > 15) = 1 - P(X ≤ 15) = 1 - Σ[(10^k × e^(-10)) / k!] for k = 0 to 15</p>
                    </div>
                    
                    <h4>Geometric Distribution</h4>
                    <p>The geometric distribution models the number of failures before the first success in a sequence of independent Bernoulli trials.</p>
                    <div class="math-formula">
                        P(X = k) = (1-p)^k × p for k = 0, 1, 2, ...
                    </div>
                    <p>where p is the probability of success on each trial.</p>
                    <p>Mean: μ = (1-p)/p</p>
                    <p>Variance: σ² = (1-p)/p²</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Geometric Distribution</div>
                        <p>A basketball player makes shots with probability 0.4. Let X be the number of misses before the first successful shot. X follows a geometric distribution with p = 0.4.</p>
                        <p>The probability of missing exactly 2 shots before making one is:</p>
                        <p>P(X = 2) = (1-0.4)^2 × 0.4 = 0.6^2 × 0.4 = 0.36 × 0.4 = 0.144</p>
                        <p>Mean: μ = (1-0.4)/0.4 = 0.6/0.4 = 1.5</p>
                        <p>On average, the player will miss 1.5 shots before making one.</p>
                    </div>
                </div>
                
                <div class="topic-section edu-card">
                    <h3 class="topic-header">Common Continuous Distributions</h3>
                    
                    <h4>Uniform Distribution</h4>
                    <p>The uniform distribution describes a random variable with constant probability density over an interval [a, b].</p>
                    <div class="math-formula">
                        f(x) = 1/(b-a) for a ≤ x ≤ b, and f(x) = 0 otherwise
                    </div>
                    <p>Mean: μ = (a+b)/2</p>
                    <p>Variance: σ² = (b-a)²/12</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Uniform Distribution</div>
                        <p>A random number generator produces values uniformly between 0 and 10. The probability of generating a number between 3 and 7 is:</p>
                        <p>P(3 ≤ X ≤ 7) = ∫[3,7] (1/10) dx = (1/10) × (7-3) = 4/10 = 0.4</p>
                        <p>Mean: μ = (0+10)/2 = 5</p>
                        <p>Variance: σ² = (10-0)²/12 = 100/12 ≈ 8.33</p>
                    </div>
                    
                    <h4>Normal (Gaussian) Distribution</h4>
                    <p>The normal distribution is a bell-shaped continuous probability distribution that is symmetric about its mean μ, with variance σ².</p>
                    <div class="math-formula">
                        f(x) = (1/(σ√(2π))) × e^(-(x-μ)²/(2σ²)) for -∞ < x < ∞
                    </div>
                    <p>The standard normal distribution has μ = 0 and σ = 1, often denoted as Z ~ N(0,1).</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Normal Distribution</div>
                        <p>The heights of Mongolian horses in a herd follow a normal distribution with mean μ = 1.4 meters and standard deviation σ = 0.1 meters.</p>
                        <p>The probability that a randomly selected horse is taller than 1.5 meters is:</p>
                        <p>P(X > 1.5) = P(Z > (1.5-1.4)/0.1) = P(Z > 1) ≈ 0.1587</p>
                        <p>where Z is the standard normal random variable. This means about 15.87% of horses in this herd are taller than 1.5 meters.</p>
                    </div>
                    
                    <h4>Exponential Distribution</h4>
                    <p>The exponential distribution models the time between events in a Poisson process, where events occur continuously and independently at a constant average rate.</p>
                    <div class="math-formula">
                        f(x) = λe^(-λx) for x ≥ 0, and f(x) = 0 for x < 0
                    </div>
                    <p>where λ is the rate parameter.</p>
                    <p>Mean: μ = 1/λ</p>
                    <p>Variance: σ² = 1/λ²</p>
                    
                    <div class="example">
                        <div class="example-title">Example: Exponential Distribution</div>
                        <p>The time between traditional Mongolian wrestling matches follows an exponential distribution with an average of 20 minutes between matches (λ = 1/20 = 0.05).</p>
                        <p>The probability that the next match starts within 15 minutes is:</p>
                        <p>P(X ≤ 15) = 1 - e^(-0.05 × 15) = 1 - e^(-0.75) ≈ 1 - 0.4724 = 0.5276</p>
                        <p>So there's about a 53% chance that the next match will start within 15 minutes.</p>
                    </div>
                </div>
                
                <div class="topic-section edu-card">
                    <h3 class="topic-header">Joint Distributions and Independence</h3>
                    
                    <h4>Joint Probability Distributions</h4>
                    <p>A joint probability distribution gives the probability of two or more random variables taking on specific values simultaneously.</p>
                    
                    <p>For discrete random variables X and Y, the joint probability mass function is:</p>
                    <div class="math-formula">
                        p(x, y) = P(X = x, Y = y)
                    </div>
                    
                    <p>For continuous random variables X and Y, the joint probability density function f(x, y) satisfies:</p>
                    <div class="math-formula">
                        P(a ≤ X ≤ b, c ≤ Y ≤ d) = ∫[a,b]∫[c,d] f(x, y) dy dx
                    </div>
                    
                    <h4>Marginal Distributions</h4>
                    <p>The marginal distribution of one variable can be obtained from the joint distribution by summing or integrating over all possible values of the other variables.</p>
                    
                    <p>For discrete random variables:</p>
                    <div class="math-formula">
                        p<sub>X</sub>(x) = Σ<sub>y</sub> p(x, y)
                    </div>
                    
                    <p>For continuous random variables:</p>
                    <div class="math-formula">
                        f<sub>X</sub>(x) = ∫ f(x, y) dy
                    </div>
                    
                    <h4>Independence of Random Variables</h4>
                    <p>Two random variables X and Y are independent if knowing the value of one does not affect the probability distribution of the other.</p>
                    
                    <p>For discrete random variables, X and Y are independent if and only if:</p>
                    <div class="math-formula">
                        p(x, y) = p<sub>X</sub>(x) × p<sub>Y</sub>(y) for all x, y
                    </div>
                    
                    <p>For continuous random variables, X and Y are independent if and only if:</p>
                    <div class="math-formula">
                        f(x, y) = f<sub>X</sub>(x) × f<sub>Y</sub>(y) for all x, y
                    </div>
                    
                    <div class="example">
                        <div class="example-title">Example: Joint Distribution</div>
                        <p>In a traditional Mongolian shagai game, let X be the number of sheep obtained in the first throw of four bones, and Y be the number of sheep obtained in the second throw.</p>
                        <p>The joint PMF is p(x, y) = (1/4)^(x+y) × (3/4)^(8-x-y) × C(4,x) × C(4,y) for x, y ∈ {0, 1, 2, 3, 4}.</p>
                        <p>The marginal PMF of X is p<sub>X</sub>(x) = Σ<sub>y</sub> p(x, y) = C(4,x) × (1/4)^x × (3/4)^(4-x) for x ∈ {0, 1, 2, 3, 4}.</p>
                        <p>Similarly, p<sub>Y</sub>(y) = C(4,y) × (1/4)^y × (3/4)^(4-y) for y ∈ {0, 1, 2, 3, 4}.</p>
                        <p>Since p(x, y) = p<sub>X</sub>(x) × p<sub>Y</sub>(y), the random variables X and Y are independent.</p>
                    </div>
                </div>
                
                <div class="navigation">
                    <a href="conditional-probability.html" class="prev-next-btn probability-btn">Previous: Conditional Probability</a>
                    <a href="expectation-and-variance.html" class="prev-next-btn probability-btn">Next: Expectation & Variance</a>
                </div>
            </div>
        </section>

        <!-- Footer -->
        <footer id="footer">
            <div id="copyright">
                <ul class="links">
                    <li>&copy; International Math Hub</li>
                </ul>
            </div>
        </footer>
    </div>

    <!-- Scripts -->
    <script src="../../assets/js/jquery.min.js"></script>
    <script src="../../assets/js/jquery.dropotron.min.js"></script>
    <script src="../../assets/js/jquery.scrollex.min.js"></script>
    <script src="../../assets/js/browser.min.js"></script>
    <script src="../../assets/js/breakpoints.min.js"></script>
    <script src="../../assets/js/util.js"></script>
    <script src="../../assets/js/main.js"></script>
</body>
</html> 